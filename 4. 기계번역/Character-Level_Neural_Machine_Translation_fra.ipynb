{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Character-Level_Neural_Machine_Translation_fra.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyN+lUFABsDkQDejr2kdsyki"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"b3L_41s6FxZw","colab_type":"code","colab":{}},"source":["### Copy from https://wikidocs.net/24996"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOUVrEHXF4De","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"abcc89a4-4dfa-403c-f8a2-a7a230478210","executionInfo":{"status":"ok","timestamp":1588234068635,"user_tz":-540,"elapsed":1967,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["import pandas as pd\n","lines= pd.read_csv('fra.txt', names=['src', 'tar','etc'], sep='\\t')\n","len(lines)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["175623"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"kGHxJ5EsF_k3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"246dea20-9f89-4fce-cf10-79695fa41c82","executionInfo":{"status":"ok","timestamp":1588234068636,"user_tz":-540,"elapsed":1956,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["lines = lines[0:60000] # 6만개만 저장\n","lines.tail()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>src</th>\n","      <th>tar</th>\n","      <th>etc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>59995</th>\n","      <td>Give me a coffee, please.</td>\n","      <td>Donnez-moi un café, s'il vous plait.</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n","    </tr>\n","    <tr>\n","      <th>59996</th>\n","      <td>Give me a coffee, please.</td>\n","      <td>Donnez-moi un café, je vous prie.</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n","    </tr>\n","    <tr>\n","      <th>59997</th>\n","      <td>Give me a glass of water.</td>\n","      <td>Donnez-moi un verre d'eau, s'il vous plaît.</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n","    </tr>\n","    <tr>\n","      <th>59998</th>\n","      <td>Give me a hand with this.</td>\n","      <td>Donne-moi un coup de main pour ça.</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n","    </tr>\n","    <tr>\n","      <th>59999</th>\n","      <td>Give me a hand with this.</td>\n","      <td>Donnez-moi un coup de main pour ceci.</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             src  ...                                                etc\n","59995  Give me a coffee, please.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n","59996  Give me a coffee, please.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n","59997  Give me a glass of water.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n","59998  Give me a hand with this.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n","59999  Give me a hand with this.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"Gv9j4yMqF_nP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":359},"outputId":"c70218e5-de2d-42e7-8363-ce7f46885afc","executionInfo":{"status":"ok","timestamp":1588234068636,"user_tz":-540,"elapsed":1945,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n","lines.sample(10)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>src</th>\n","      <th>tar</th>\n","      <th>etc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>13317</th>\n","      <td>You're powerful.</td>\n","      <td>\\t Vous êtes puissant. \\n</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n","    </tr>\n","    <tr>\n","      <th>36687</th>\n","      <td>Mary is Tom's cousin.</td>\n","      <td>\\t Mary est la cousine de Tom. \\n</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n","    </tr>\n","    <tr>\n","      <th>59315</th>\n","      <td>Are you writing a letter?</td>\n","      <td>\\t Es-tu en train d'écrire une lettre? \\n</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n","    </tr>\n","    <tr>\n","      <th>28159</th>\n","      <td>Does that scare you?</td>\n","      <td>\\t Est-ce que cela vous effraie ? \\n</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n","    </tr>\n","    <tr>\n","      <th>47864</th>\n","      <td>I know I've helped you.</td>\n","      <td>\\t Je sais que je t'ai aidé. \\n</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n","    </tr>\n","    <tr>\n","      <th>56912</th>\n","      <td>The lock must be broken.</td>\n","      <td>\\t Il faut casser la serrure. \\n</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n","    </tr>\n","    <tr>\n","      <th>10012</th>\n","      <td>Boy was I naive.</td>\n","      <td>\\t Dieu que j'étais naïve ! \\n</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n","    </tr>\n","    <tr>\n","      <th>47356</th>\n","      <td>I came with my friends.</td>\n","      <td>\\t Je vins avec mes amies. \\n</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n","    </tr>\n","    <tr>\n","      <th>13007</th>\n","      <td>What scared you?</td>\n","      <td>\\t Qu'est-ce qui vous a effrayées ? \\n</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n","    </tr>\n","    <tr>\n","      <th>35160</th>\n","      <td>I knew I had to stop.</td>\n","      <td>\\t Je savais que je devais arrêter. \\n</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             src  ...                                                etc\n","13317           You're powerful.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n","36687      Mary is Tom's cousin.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #1...\n","59315  Are you writing a letter?  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #1...\n","28159       Does that scare you?  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n","47864    I know I've helped you.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n","56912   The lock must be broken.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n","10012           Boy was I naive.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n","47356    I came with my friends.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n","13007           What scared you?  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #1...\n","35160      I knew I had to stop.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n","\n","[10 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"W9rZ78j-F_px","colab_type":"code","colab":{}},"source":["# 글자 집합 구축\n","src_vocab=set()\n","for line in lines.src: # 1줄씩 읽음\n","    for char in line: # 1개의 글자씩 읽음\n","        src_vocab.add(char)\n","\n","tar_vocab=set()\n","for line in lines.tar:\n","    for char in line:\n","        tar_vocab.add(char)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Cp85pOrF_sW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"e1aa456a-b176-4d75-9324-5872552fe8c1","executionInfo":{"status":"ok","timestamp":1588234068637,"user_tz":-540,"elapsed":1929,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["src_vocab_size = len(src_vocab)+1\n","tar_vocab_size = len(tar_vocab)+1\n","print(src_vocab_size)\n","print(tar_vocab_size)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["79\n","106\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L6wA7yYCF_uo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"e104de46-589c-421a-b0e3-74709cb34676","executionInfo":{"status":"ok","timestamp":1588234068637,"user_tz":-540,"elapsed":1860,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["src_vocab = sorted(list(src_vocab))\n","tar_vocab = sorted(list(tar_vocab))\n","print(src_vocab[45:75])\n","print(tar_vocab[45:75])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zg4XF0S2F_w0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"b4e2a16e-0c62-4cd4-fbb9-c2c31437518c","executionInfo":{"status":"ok","timestamp":1588234068638,"user_tz":-540,"elapsed":1846,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n","tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n","print(src_to_index)\n","print(tar_to_index)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n","{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, 'С': 100, '\\u2009': 101, '\\u200b': 102, '‘': 103, '’': 104, '\\u202f': 105}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y8xJ--iJF_zk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bc27c2e5-d876-4225-fcaf-d92a7eea9eae","executionInfo":{"status":"ok","timestamp":1588234069172,"user_tz":-540,"elapsed":2370,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["encoder_input = []\n","for line in lines.src: #입력 데이터에서 1줄씩 문장을 읽음\n","    temp_X = []\n","    for w in line: #각 줄에서 1개씩 글자를 읽음\n","      temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n","    encoder_input.append(temp_X)\n","print(encoder_input[:5])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[[30, 64, 10], [31, 58, 10], [31, 58, 10], [41, 70, 63, 2], [41, 70, 63, 2]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bf8F2zPSF_1u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6376ff80-84b6-46db-8ab1-11449c3f9707","executionInfo":{"status":"ok","timestamp":1588234069531,"user_tz":-540,"elapsed":2721,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["decoder_input = []\n","for line in lines.tar:\n","    temp_X = []\n","    for w in line:\n","      temp_X.append(tar_to_index[w])\n","    decoder_input.append(temp_X)\n","print(decoder_input[:5])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2], [1, 3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [1, 3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"etynIYFvGfII","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"83d7e2cd-183f-42f0-e8dc-48b4929f348e","executionInfo":{"status":"ok","timestamp":1588234070061,"user_tz":-540,"elapsed":3235,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["decoder_target = []\n","for line in lines.tar:\n","    t=0\n","    temp_X = []\n","    for w in line:\n","      if t>0:\n","        temp_X.append(tar_to_index[w])\n","      t=t+1\n","    decoder_target.append(temp_X)\n","print(decoder_target[:5])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[[3, 48, 53, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2], [3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bXVN-KIdGhy-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"720b834b-fbb2-46dc-8ff8-11fc34532274","executionInfo":{"status":"ok","timestamp":1588234070061,"user_tz":-540,"elapsed":3226,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["max_src_len = max([len(line) for line in lines.src])\n","max_tar_len = max([len(line) for line in lines.tar])\n","print(max_src_len)\n","print(max_tar_len)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["25\n","76\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ayrBZQLhGjiB","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n","decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n","decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Swi6r6MeGsbG","colab_type":"code","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","encoder_input = to_categorical(encoder_input)\n","decoder_input = to_categorical(decoder_input)\n","decoder_target = to_categorical(decoder_target)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1EdCIqv0GtZj","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n","from tensorflow.keras.models import Model\n","\n","encoder_inputs = Input(shape=(None, src_vocab_size))\n","encoder_lstm = LSTM(units=256, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n","# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n","encoder_states = [state_h, state_c]\n","# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eoKfauJmGusV","colab_type":"code","colab":{}},"source":["decoder_inputs = Input(shape=(None, tar_vocab_size))\n","decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n","decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n","decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n","decoder_outputs = decoder_softmax_layer(decoder_outputs)\n","\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5tGAqiSqGxv2","colab_type":"code","colab":{}},"source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ry8G2FGbHbnb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":697},"outputId":"941d9b1b-c999-4390-c420-6715f90ad716","executionInfo":{"status":"ok","timestamp":1588238561042,"user_tz":-540,"elapsed":4449623,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=.2, callbacks=[es])"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","750/750 [==============================] - 233s 311ms/step - loss: 0.7701 - acc: 0.7902 - val_loss: 0.6850 - val_acc: 0.7979\n","Epoch 2/50\n","750/750 [==============================] - 232s 310ms/step - loss: 0.4776 - acc: 0.8576 - val_loss: 0.5545 - val_acc: 0.8343\n","Epoch 3/50\n","750/750 [==============================] - 232s 309ms/step - loss: 0.4014 - acc: 0.8796 - val_loss: 0.4924 - val_acc: 0.8522\n","Epoch 4/50\n","750/750 [==============================] - 235s 313ms/step - loss: 0.3563 - acc: 0.8923 - val_loss: 0.4538 - val_acc: 0.8636\n","Epoch 5/50\n","750/750 [==============================] - 233s 310ms/step - loss: 0.3262 - acc: 0.9011 - val_loss: 0.4279 - val_acc: 0.8708\n","Epoch 6/50\n","750/750 [==============================] - 236s 315ms/step - loss: 0.3043 - acc: 0.9076 - val_loss: 0.4082 - val_acc: 0.8767\n","Epoch 7/50\n","750/750 [==============================] - 234s 312ms/step - loss: 0.2873 - acc: 0.9126 - val_loss: 0.3944 - val_acc: 0.8812\n","Epoch 8/50\n","750/750 [==============================] - 233s 310ms/step - loss: 0.2737 - acc: 0.9165 - val_loss: 0.3850 - val_acc: 0.8840\n","Epoch 9/50\n","750/750 [==============================] - 233s 310ms/step - loss: 0.2623 - acc: 0.9198 - val_loss: 0.3772 - val_acc: 0.8865\n","Epoch 10/50\n","750/750 [==============================] - 233s 311ms/step - loss: 0.2525 - acc: 0.9226 - val_loss: 0.3731 - val_acc: 0.8879\n","Epoch 11/50\n","750/750 [==============================] - 232s 310ms/step - loss: 0.2440 - acc: 0.9250 - val_loss: 0.3707 - val_acc: 0.8893\n","Epoch 12/50\n","750/750 [==============================] - 233s 311ms/step - loss: 0.2363 - acc: 0.9273 - val_loss: 0.3658 - val_acc: 0.8905\n","Epoch 13/50\n","750/750 [==============================] - 231s 308ms/step - loss: 0.2295 - acc: 0.9292 - val_loss: 0.3639 - val_acc: 0.8915\n","Epoch 14/50\n","750/750 [==============================] - 233s 310ms/step - loss: 0.2233 - acc: 0.9310 - val_loss: 0.3616 - val_acc: 0.8927\n","Epoch 15/50\n","750/750 [==============================] - 236s 314ms/step - loss: 0.2176 - acc: 0.9326 - val_loss: 0.3595 - val_acc: 0.8930\n","Epoch 16/50\n","750/750 [==============================] - 236s 315ms/step - loss: 0.2123 - acc: 0.9342 - val_loss: 0.3605 - val_acc: 0.8936\n","Epoch 17/50\n","750/750 [==============================] - 236s 315ms/step - loss: 0.2074 - acc: 0.9356 - val_loss: 0.3618 - val_acc: 0.8935\n","Epoch 18/50\n","750/750 [==============================] - 233s 311ms/step - loss: 0.2029 - acc: 0.9369 - val_loss: 0.3619 - val_acc: 0.8940\n","Epoch 19/50\n","750/750 [==============================] - 233s 311ms/step - loss: 0.1986 - acc: 0.9383 - val_loss: 0.3626 - val_acc: 0.8939\n","Epoch 00019: early stopping\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fab1afe9ba8>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"VctMojsrGzXt","colab_type":"code","colab":{}},"source":["encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OTHhEZmQHIo7","colab_type":"code","colab":{}},"source":["# 이전 시점의 상태들을 저장하는 텐서\n","decoder_state_input_h = Input(shape=(256,))\n","decoder_state_input_c = Input(shape=(256,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n","decoder_states = [state_h, state_c]\n","# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n","decoder_outputs = decoder_softmax_layer(decoder_outputs)\n","decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fZkJ0ba8HIrX","colab_type":"code","colab":{}},"source":["index_to_src = dict((i, char) for char, i in src_to_index.items())\n","index_to_tar = dict((i, char) for char, i in tar_to_index.items())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3njId8l6HItj","colab_type":"code","colab":{}},"source":["def decode_sequence(input_seq):\n","    # 입력으로부터 인코더의 상태를 얻음\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # <SOS>에 해당하는 원-핫 벡터 생성\n","    target_seq = np.zeros((1, 1, tar_vocab_size))\n","    target_seq[0, 0, tar_to_index['\\t']] = 1.\n","\n","    stop_condition = False\n","    decoded_sentence = \"\"\n","\n","    # stop_condition이 True가 될 때까지 루프 반복\n","    while not stop_condition:\n","        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","        # 예측 결과를 문자로 변환\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = index_to_tar[sampled_token_index]\n","\n","        # 현재 시점의 예측 문자를 예측 문장에 추가\n","        decoded_sentence += sampled_char\n","\n","        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_tar_len):\n","            stop_condition = True\n","\n","        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n","        target_seq = np.zeros((1, 1, tar_vocab_size))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n","        states_value = [h, c]\n","\n","    return decoded_sentence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AXZMIR4QHMuZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"1efa7f36-6ca3-4e9a-9dee-81d0c4db04aa","executionInfo":{"status":"ok","timestamp":1588238565657,"user_tz":-540,"elapsed":4442226,"user":{"displayName":"김선진","photoUrl":"","userId":"00248900647890093087"}}},"source":["import numpy as np\n","for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n","    input_seq = encoder_input[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print(35 * \"-\")\n","    print('입력 문장:', lines.src[seq_index])\n","    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n","    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"],"execution_count":25,"outputs":[{"output_type":"stream","text":["-----------------------------------\n","입력 문장: Run!\n","정답 문장:  Cours ! \n","번역기가 번역한 문장:  Suivez ça ! \n","-----------------------------------\n","입력 문장: I lied.\n","정답 문장:  J'ai menti. \n","번역기가 번역한 문장:  Je l'ai appris. \n","-----------------------------------\n","입력 문장: Come in.\n","정답 문장:  Entre. \n","번역기가 번역한 문장:  Venez ! \n","-----------------------------------\n","입력 문장: I did OK.\n","정답 문장:  Je m'en suis bien sortie. \n","번역기가 번역한 문장:  Je m'en suis bien sorti. \n","-----------------------------------\n","입력 문장: We're sad.\n","정답 문장:  Nous sommes tristes. \n","번역기가 번역한 문장:  Nous sommes des reches. \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h5bWPx3rIXhL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}